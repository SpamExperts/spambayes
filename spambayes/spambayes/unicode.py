#! /usr/bin/env python

"""Package to handle unicodified text.

Some MUA and programs generate messages lacking exact character set
information (e.g. no charset parameter, unencoded non-ASCII header),
and some spammers try to fake character set of message to through
content filters.

The following functions help decoding strings to unicode, detecting
a suitable character set.

    utext(s, [charset], [candidate_charset])
        Decode text to unicode.  Charset will first be tested.
        If auto-detection fails, candidate_charset will be tested.

    uheader(s, [candidate_charset])
        Decode MIME header to unicode.  If auto-detection fails,
        candidate_charset will be tested.

    ucanonical(s)
        Canonicalize unicode text s by Normalization From KC (NFKC).

    get_message_charset(msg, [failobj])
        Detect preferred charset of message object msg.
        If detection failed, failobj will be returned.
"""

import types
import codecs
import htmllib
import binascii
import formatter
from sgmllib import SGMLParseError

import email
try:
    import email.errors
except ImportError:
    # Handle Python 2.4
    import email.Errors as email_errors
    email.errors = email_errors
    del email_errors
try:
    import email.header
except ImportError:
    # Handle Python 2.4
    import email.Header as email_header
    email.header = email_header
    del email_header
try:
    import email.charset
except ImportError:
    # Handle Python 2.4
    import email.Charset as email_charset
    email.charset = email_charset
    del email_charset

try:
    import unicodedata
    if not hasattr(unicodedata, 'normalize'):
        raise ImportError
except ImportError:
    import compatunicodedata as unicodedata

try:
    True, False
except NameError:
    # Maintain compatibility with Python 2.2
    True, False = 1, 0

__all__ = ('utext', 'uheader', 'ucanonical', 'get_message_charset')

# Add unofficial charset aliases
for nonstd, std in (
    # ASCII
    ('ansi_x3.4-1968', 'us-ascii'),
    # Chinese
    ('gb2312_charset', 'gb2312'),
    # Japanese
    ('x-sjis',         'shift_jis'),
    ('x-euc-jp',       'euc-jp'),
    ('x-eucjp',        'euc-jp'),
    # Korean
    ('ks_c_5601',      'ks_c_5601-1987'),
    # Thai
    ('windows-874',    'cp874'),
    # Unicode
    ('unicode-1-1-utf-7',    'utf-7'),
    ):
    if not email.charset.ALIASES.has_key(nonstd):
        email.charset.add_alias(nonstd, std)


DEFAULT_CHARSET = 'iso-8859-1'
# Default charsets of email package.  They often are not real charset.
EMAIL_DEFAULT_CHARSETS = (None, email.charset.DEFAULT_CHARSET)
# Fallback charsets generated by auto-detection.
AUTODET_DEFAULT_CHARSETS = EMAIL_DEFAULT_CHARSETS + (DEFAULT_CHARSET,)
# Default content charsets of HTML. They often don't describe real charset.
HTTP_DEFAULT_CHARSETS = EMAIL_DEFAULT_CHARSETS + ('iso-8859-1', 'windows-1252')

# Escape sequences for ISO-2022-* encoding schema.
ESCAPE_SEQUENCES = {
    ## Multibyte sequences.
    # JIS C 6226-1978
    "\033$@":  ['iso-2022-jp.ext', 'iso-2022-jp', 'iso-2022-jp-2',],
    # GB2312-1980 (G1DM4)
    "\033$)A": ['iso-2022-cn-ext', 'iso-2022-cn',],
    # JIS X 0208-1983
    "\033$B":  ['iso-2022-jp.ext', 'iso-2022-jp', 'iso-2022-jp-2',
                'iso-2022-jp-3', 'iso-2022-jp-2004',],
    # KS C 5601-1987 (G1DM4)
    "\033$)C": ['iso-2022-kr',],
    # JIS X 0212-1990
    "\033$(D": ['iso-2022-jp.ext', 'iso-2022-jp-2',],
    # CNS-11643 plane 1 (G1DM4)
    "\033$)G": ['iso-2022-cn-ext', 'iso-2022-cn',],
    # CNS-11643 plane 2 (G2DM4)
    "\033$*H": ['iso-2022-cn-ext', 'iso-2022-cn',],

    ## Single-byte sequences.
    # ASCII
    "\033(B":  ['iso-2022-jp.ext', 'iso-2022-jp', 'iso-2022-jp-2',],
    # JIS X 0201 katakana
    "\033(I":  ['iso-2022-jp.ext', 'iso-2022-jp',],
    # JIS X 0201 roman
    "\033(J":  ['iso-2022-jp.ext', 'iso-2022-jp', 'iso-2022-jp-2',],

    # Following sequences are rarely used.  When they are used,
    # charset paramater shall be specified in message header.

    ## Multibyte sequences.
    # GB2312-1980
    "\033$A":  ['iso-2022-jp-2',],
    # JIS X 0208-1990 (with update bytes)
    "\033&@\033$B": ['iso-2022-jp',],
    # KS C 5601-1987
    "\033$(C": ['iso-2022-jp-2',],
    # JIS X 0213:2000 plane 1
    "\033$(O": ['iso-2022-jp-3',],
    # JIS X 0213:2000/2004 plane 2
    "\033$(P": ['iso-2022-jp-2004', 'iso-2022-jp-3',],
    # JIS X 0213:2004 plane 1
    "\033$(Q": ['iso-2022-jp-2004',],

    ## Single-byte sequences.
    # ISO 8859-1
    "\033.A":  ['iso-2022-jp-2',],
    # ISO 8859-7
    "\033.F":  ['iso-2022-jp-2',],
}

# Some spams use different character encoding scheme from one of charset name.
# Note: charset names should be ordered by extended-first.
ALTERNATIVE_CODECS = {
    'iso-2022-jp':      ['cp932', 'shift_jis',],
}

# Some messages use wider coded character set than one of charset name.
# Note: charset names should be ordered by extended-first.
COMPATIBLE_CODECS = {
    # cjkcodecs has 'gbk' (cp936) codec but ChineseCodecs doesn't.
    'euc-cn':           ['gbk', 'gb2312',],
    'gb2312':           ['gbk', 'gb2312',],
    # JapaneseCodecs has 'iso_2022_jp_ext' codec (includes JIS X 0201 GR)
    # but earlier version of cjkcodecs doesn't.
    'iso-2022-jp':      ['iso-2022-jp.ext', 'iso-2022-jp',],
    'shift_jis':        ['cp932', 'shift_jis',],
    'euc-kr':           ['cp949', 'euc-kr',],
    'ks_c_5601-1987':   ['cp949', 'euc-kr',],
    'iso-8859-1':       ['windows-1252', 'iso-8859-1',],
    # Python <= 2.3 doesn't have tis-620 and iso-8859-11 codecs.
    'tis-620':          ['cp874', 'tis-620'],
    'iso-8859-11':      ['cp874', 'tis-620'],
}

# Some of ISO-646 domestic variants (i.e. Japan and Korea) assign
# national currency symbol to 5/12.  Some codepages map '\x5C' to
# REVERSE SOLIDUS.
NF_CURRENCY_MAP = {
    0x005C: u'\u00A4', # CURRENCY SIGN
}

def almost_8bit(text, percentage=0.9):
   """Helper function to determine if the text is almost 7-bit or not."""
   if not isinstance(text, types.StringType):
       return False
   t = [c for c in text if c < '\x80']
   return len(t) < len(text) * percentage

def detect_7bit_charset(text):
    """Detect ISO-2022-* or HZ charset.

    If escape sequences are found and conversion sucseeded, return
    charset name.  If text is 7-bit, return 'us-ascii'. Otherwise,
    return None."""
    if almost_8bit(text):
        return None

    # Search escape sequences and try converting.
    csets = []
    for eseq in ESCAPE_SEQUENCES.keys():
        if text.find(eseq) >= 0:
            for cset in ESCAPE_SEQUENCES[eseq]:
                if cset not in csets:
                    try:
                        unicode(text, cset, 'strict')
                        return cset
                    # XXX A more explicit 'except' should be used here.
                    except:
                        pass
                    csets.append(cset)
    # Some escape sequences are found but conversion failed.
    # The text contains nonstandard character(s). Try replacing.
    for cset in csets:
        try:
            unicode(text, cset, 'replace')
            return cset
        # XXX A more explicit 'except' should be used here.
        except:
            pass
    # Try HZ.
    if text.find('\033') < 0 and \
        text.find('~{') >= 0 and text.find('~}') >= 0:
        try:
            unicode(text, 'hz-gb-2312', 'strict')
            return 'hz-gb-2312'
        # XXX A more explicit 'except' should be used here.
        except:
            pass
    # No charsets are detected. The text is US-ASCII or 8-bit.
    try:
        unicode(text, 'us-ascii', 'strict')
    # XXX A more explicit 'except' should be used here.
    except:
        return None
    return 'us-ascii'

def official_charset(charset, verify_codecs=True):
    """Get official charset name."""
    if not charset:
        return None
    if not isinstance(charset, email.charset.Charset):
        if isinstance(charset, types.UnicodeType):
            charset = charset.encode('latin-1')
        try:
            charset = email.charset.Charset(charset)
        except email.errors.CharsetError:
            return None
    charset = charset.input_charset.lower()
    # Verify codecs.
    if verify_codecs:
        try:
            codecs.lookup(charset)
        except (AttributeError, LookupError, ValueError):
            return None
    return charset


class CharsetParser(htmllib.HTMLParser):
    """Class to parse HTML text and to determine charset from META
    information."""
    def __init__(self):
        htmllib.HTMLParser.__init__(self, formatter.NullFormatter())
        self.charset = None

    def do_meta(self, attrs):
        d = {}
        for k, v in attrs:
            d[k] = v
        if d.get('http-equiv', '').lower() != 'content-type':
            return
        s = d.get('content')
        if not s:
            return
        m = email.message_from_string('content-type: '+s+'\r\n\r\n')
        charset = official_charset(m.get_content_charset())
        if charset:
            self.charset = charset


class UnicodifiedText(email.header.Header):
    """Class to parse MIME header or string and to decode to Unicode
    using most preferred charset."""
    def __init__(self, s=None, charset=None, other_charset=None,
                 decode_header=False, analyze_html=False,
                 charset_only=False):
        email.header.Header.__init__(self)
        if s is None:
            return

        # Verify codecs of charset.
        charset = official_charset(charset)
        other_charset = official_charset(other_charset)
        if not charset and other_charset:
            charset = other_charset

        # Make chunks from string.
        if isinstance(s, types.StringType):
            chunks = [(s, charset)]
            if decode_header:
                try:
                    chunks = email.header.decode_header(s)
                    if not isinstance(chunks, types.ListType):
                        chunks = [(s, charset)]
                except (binascii.Error, email.errors.HeaderParseError):
                    pass
            elif analyze_html:
                parser = CharsetParser()
                try:
                    parser.feed(s)
                    parser.close()
                except (SGMLParseError, AttributeError):
                    pass
                # Latin-1 is often inadequately specified in HTML.
                if parser.charset not in HTTP_DEFAULT_CHARSETS:
                    chunks = [(s, parser.charset)]
        elif isinstance(s, email.header.Header):
            chunks = s._chunks
        elif isinstance(s, types.UnicodeType):
            chunks = [(s, "latin-1")]
        else:
            raise ValueError('Unexpected type of object: %s' % (type(s),))

        # Decode chunks to Unicode detecting preferred charset.
        self._charset = None
        for text, cset in chunks:
            # If charset is not known, try to detect 7-bit encodings at
            # first.
            if cset in EMAIL_DEFAULT_CHARSETS:
                c = detect_7bit_charset(text)
                if c:
                    self.append(text, c, 'replace')
                    if self._charset and charset_only:
                        return
                    continue

            # If charset is specified, try decoding by alternative codecs,
            # compatible codecs and then standard codec.
            found = False
            cset = official_charset(cset, False) # reserve unknown codec
            if cset not in EMAIL_DEFAULT_CHARSETS:
                if almost_8bit(text):
                    for c in ALTERNATIVE_CODECS.get(cset, []):
                        try:
                            self.append(text, c)
                            if charset_only: return
                            found = True
                            break
                        # XXX A more explicit 'except' should be used here.
                        except:
                            pass
                    if found: continue
                for c in COMPATIBLE_CODECS.get(cset, []):
                    try:
                        self.append(text, c)
                        # Set original charset to refer compatible codecs
                        # at next time.
                        self.setCharset(cset)
                        if charset_only: return
                        found = True
                        break
                    # XXX A more explicit 'except' should be used here.
                    except:
                        pass
                if found: continue
                try:
                    self.append(text, cset, 'replace')
                    if self._charset and charset_only: return
                    continue
                # XXX A more explicit 'except' should be used here.
                except:
                    pass
            # Otherwise, try candidate charset.
            if other_charset:
                try:
                    self.append(text, other_charset)
                    if self._charset and charset_only: return
                    continue
                # XXX A more explicit 'except' should be used here.
                except:
                    pass
            # Give up!  Use default charset but not set it.
            cset = self._charset
            self.append(text, DEFAULT_CHARSET, 'replace')
            self._charset = cset

        # Charset detection was failed. For convenience, set ISO-8859-1.
        if not self._charset:
            self.setCharset(DEFAULT_CHARSET)

    def append(self, s, charset=None, errors='strict'):
        """Append a string to chunks and set charset."""
        if errors == 'strict':
            email.header.Header.append(self, s, charset)
        else:
            s = unicode(s, charset, errors)
            email.header.Header.append(self, s, email.header.UTF8)
        if not self._charset:
            self.setCharset(charset)

    def setCharset(self, charset=None):
        if not isinstance(charset, email.charset.Charset):
            charset = email.charset.Charset(charset)
        if charset not in EMAIL_DEFAULT_CHARSETS:
            self._charset = charset


def utext(s, charset=None, candidate_charset=None):
    """Unicodify body text detecting charset."""
    if not s or isinstance(s, types.UnicodeType):
        return s
    ut = UnicodifiedText(s, charset=charset,
                         other_charset=candidate_charset)
    return unicode(ut)

def uheader(s, candidate_charset=None):
    """Unicodify header text detecting charset."""
    if not s or isinstance(s, types.UnicodeType):
        return s
    ut = UnicodifiedText(s, other_charset=candidate_charset,
                         decode_header=True)
    return unicode(ut)

def ucanonical(s, currencyfix=True):
    """Some characters of standards and vendor codepages may have
    variants (e.g. fullwidth/halfwidth forms), duplicated glyphs
    (e.g. compatibility ideographs) etc. in Unicode Standards.
    Canonicalize them."""
    if not s or not isinstance(s, types.UnicodeType):
        return s
    if currencyfix:
        s = s.translate(NF_CURRENCY_MAP)
    return unicodedata.normalize('NFKC', s)

def get_message_charset(msg, failobj=DEFAULT_CHARSET):
    """Determine preferred charset of a message object."""
    # Search non-US-ASCII text part.
    for part in msg.walk():
        # Ignore non-text part.
        if part.get_content_maintype() != 'text':
            continue
        charset = official_charset(part.get_content_charset())

        try:
            payload = part.get_payload(decode=True)
        # XXX A more explicit 'except' should be used here.
        except:
            payload = part.get_payload(decode=False)
        if not payload:
            continue

        # If no charset is specified and type is HTML, get META
        # information.
        if charset in HTTP_DEFAULT_CHARSETS and \
            part.get_content_type() == 'text/html':
            ut = UnicodifiedText(payload, analyze_html=True,
                                 charset_only=True)
            if ut._charset not in HTTP_DEFAULT_CHARSETS:
                return ut._charset
        # Otherwise, auto-detect.
        else:
            ut = UnicodifiedText(payload, charset=charset,
                                 charset_only=True)
            if ut._charset not in AUTODET_DEFAULT_CHARSETS:
                return ut._charset

    # Search headers.
    for h in ('subject', 'from', 'to'):
        v = msg.get(h, None)
        if v:
            ut = UnicodifiedText(v, decode_header=True, charset_only=True)
            if ut._charset not in EMAIL_DEFAULT_CHARSETS:
                return ut._charset
    return failobj

# Check UCS-4 Support
last_unicode = u'\U0010FFFF'
if len(last_unicode) > 1:
    last_unicode = u'\uFFFF'

# Ranges for Unicode scripts etc.
alpha = ''                    # Alphabetic scripts.
hanzi = ''                    # HAN script.
kana =  ''                    # HIRAGANA & KATAKANA scripts.
hangul = ''                   # HANGUL script.
curr = ''                     # Currency-like symbols.
punct = ''                    # Punctuation and symbols.

######## ################ # # ##########################################
# Type   # Block          # # # Block/Character Name; Comment
######## ################ # # ##########################################
#        u'\u0000-\u007F' #   Basic Latin:
punct+=   '\x00-\x08'     #   C0 Control except whitespaces.
punct+=   '\x0E-\x1F'     #       ,,
punct+=   r'!-/:-@\[-`{-~'#   ASCII punctuation chars.
curr+=     '$'            #   DOLLAR SIGN
curr+=     '%'            #   PERCENT SIGN
curr+=     '.'            #   FULL STOP; used as decimal point.
alpha+=   r'0-9A-Z_a-z'   #   ASCII word chars.
alpha+=  u'\u0080-\u00FF' #   Latin-1 Supplement:
punct+=   u'\u0080-\u009F'# C C1 Control
punct+=   u'\u00A0-\u00BF'# C Latin-1 punctuations and symbols:
curr+=     u'\u00A2'      #   CENT SIGN
curr+=     u'\u00A3'      #   POUND SIGN
curr+=     u'\u00A4'      #   CURRENCY SIGN
curr+=     u'\u00A5'      #   YEN SIGN
punct+=   u'\u00D7'       # C MULTIPLICATION SIGN
punct+=   u'\u00F7'       # C DIVISION SIGN
alpha+=  u'\u0100-\u10FF' #   Latin Ext.-A .. Georgean
hangul+= u'\u1100-\u11FF' #   Hangul Jamo; some sequences won't be normalized.
alpha+=  u'\u1200-\u1FFF' #   Ethiopic .. Greek Ext.
punct+=  u'\u2000-\u206F' #   General Punctuation:
hanzi+=   u'\u203B'       # * REFERENCE MARK; used in Japanese UCE tag.
# ---    u'\u2070-\u209F' # - Superscripts and Subscripts
punct+=  u'\u20A0-\u20CF' #   Currency Symbols
curr+=   u'\u20A0-\u20CF' #       ,,
punct+=  u'\u20D0-\u2BFF' #   Comb. Marks for Symbols..Misc. Symbols and Arrows
alpha+=  u'\u2C00-\u2DFF' #   N/A; alphabetic script(s) in the future.
punct+=  u'\u2E00-\u2E7F' #   N/A; punctuation and symbols in the future.
hanzi+=  u'\u2E80-\u2EFF' #   CJK Radicals Supplement
# ---    u'\u2F00-\u2FDF' # - Kangxi Radicals
punct+=  u'\u2FE0-\u2FFF' #   I. Description Character
punct+=  u'\u3000-\u303F' #   CJK Symbols and Punctuation:
hanzi+=   u'\u3005'       # + I. ITERATION MARK
hanzi+=   u'\u3006'       # * I. CLOSING MARK; treated as hanzi-like in JA.
hanzi+=   u'\u3007'       # + I. NUMBER ZERO
curr+=    u'\u3012'       #   POSTAL MARK; prefixed on JP postal code (digit).
hanzi+=   u'\u3013'       # * GETA MARK; substitute for char. not in font.
kana+=    u'\u301C'       # * WAVE DASH; sometimes used as prolonged sign.
hanzi+=   u'\u3021-\u3029'# + Hangzhou Numeral
# ---     u'\u3038-\u303A'# - ditto
kana+=    u'\u3031-\u3035'# * V.KANA REPEAT MARK..V.KANA REPEAT MARK LOW HALF
hanzi+=   u'\u303B'       # + V. I. ITERATION MARK
kana+=    u'\u303C'       # * MASU MARK; used as kana abbreviation.
kana+=   u'\u3040-\u309F' #   Hiragana
kana+=   u'\u30A0-\u30FF' #   Katakana:
punct+=   u'\u30A0'       # + KATAKANA-HIRAGANA DOUBLE HYPHEN
hanzi+=   u'\u30F5'       # = KATAKANA LETTER SMALL KA; treated as hanzi-like.
hanzi+=   u'\u30F6'       # = KATAKANA LETTER SMALL KE; treated as hanzi-like.
punct+=   u'\u30FB'       # + KATAKANA MIDDLE DOT; used in kana compound word.
alpha+=  u'\u3100-\u312F' #   Bopomofo
# ---    u'\u3130-\u318F' # - Hangul Comp. Jamo
hanzi+=  u'\u3190-\u319F' # * Kanbun; normalized except:
punct+=   u'\u3190'       #   I. ANNOTATION LINKING MARK
punct+=   u'\u3191'       #   I. ANNOTATION REVERSE MARK
alpha+=  u'\u31A0-\u31BF' #   Bopomofo Ext.
punct+=  u'\u31C0-\u31EF' #   N/A
kana+=   u'\u31F0-\u31FF' #   Katakana Phonetic Ext.
punct+=  u'\u3200-\u32FF' # - Enclosed CJK Letters etc.; normalized except:
#punct    u'\u327F'       #   KOREAN STANDARD SYMBOL
# ---    u'\u3300-\u33FF' # - CJK Compatibility
hanzi+=  u'\u3400-\u4DBF' #   CJK Unified I. Ext. A
punct+=  u'\u4DC0-\u4DFF' #   Yijing Hexagram Symbols
hanzi+=  u'\u4E00-\u9FFF' #   CJK Unified I.
punct+=  u'\uA000-\uA48F' # ? Yi Syllables; I don't know how to support them.
punct+=  u'\uA490-\uA4CD' # ? Yi Radicals; I don't know how to support them.
punct+=  u'\uA4CE-\uA7FF' #   N/A
alpha+=  u'\uA800-\uABFF' #   N/A; for alphabetic script(s) in the future.
hangul+= u'\uAC00-\uD7AF' #   Hangul Syllable
punct+=  u'\uD7B0-\uD7FF' #   N/A
# XXX    u'\uD800-\uDB7F' #   High Surrogates
# XXX    u'\uDB80-\uDBFF' #   High Private Use Surrogates
# XXX    u'\uDC00-\uDFFF' #   Low Surrogates
punct+=  u'\uE000-\uF8FF' #   Private Use Area
hanzi+=  u'\uF900-\uFAFF' # - CJK Comp. I.; normalized except some chars.
alpha+=  u'\uFB00-\uFB4F' # - Alphabetic Presentation Forms; normalized except:
#alpha    u'\uFB1E'       #   HEBREW POINT JUDEO-SPANISH VARIKA
punct+=  u'\uFB50-\uFDFF' # - Arabic Presentation Forms-A; normalized excpet:
#punct    u'\uFDFD'       #   ARABIC LIGATURE BISMILLAH AR-RAHMAN AR-RAHEEM
punct+=  u'\uFE00-\uFE0F' #   Variation Selectors
punct+=  u'\uFE10-\uFE1F' #   N/A
alpha+=  u'\uFE20-\uFE2F' #   Combining Half Marks
punct+=  u'\uFE30-\uFE4F' # - CJK Comp. Forms; normalized except:
#punct    u'\uFE45'       #   SESAME DOT
#punct    u'\uFE46'       #   WHITE SESAME DOT
# ---     u'\uFE50-\uFE6F'# - Small Form Variants
alpha+=  u'\uFE70-\uFEFF' # - Arabic Presentation Forms-B; normalized except:
#alpha    u'\uFE73'       #   ARABIC TAIL FRAGMENT
#alpha    u'\uFEFF'       #   ZERO WIDTH NO-BREAK SPACE
# ---    u'\uFF00-\uFFEF' # - Halfwidth and Fullwidth Forms
punct+=  u'\uFFF0-\uFFFF' #   Specials
if last_unicode > u'\uFFFF':
    punct+=  u'\U00010000-\U0001FFFF' #   Archaeic scripts etc.; rarely used.
    hanzi+=  u'\U00020000-\U0002F7FF' #   CJK Unified I. Ext. B
    hanzi+=  u'\U0002F800-\U0002FFFF' # - CJK Comp. I. Supplement
    punct+=  u'\U00030000-\U0010FFFF' #   others

# C: This range overlaps with punctuation for compatibility with
#    the original version of spambayes.
# -: Character(s) normalized by unicodedata.normalize().
# +: Character belonged to script block and also treated as punctuation.
# =: Character(s) not belonged to script block of Unicode Standard.
# *: Character(s) not belonged to script block of Unicode Standard and
#    also treated as punctuation.
